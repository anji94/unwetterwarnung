{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unwetterwarnung.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kenzo94/unwetterwarnung/blob/master/Unwetterwarnung.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CrsUX9J5h9Q"
      },
      "source": [
        "**Unwetterwarnung mit Arduino Nano 33 BLE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNFxfMNbyk91"
      },
      "source": [
        "**1. Rohdaten von Kaggle Herunterladen**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBajN9DN1a9n"
      },
      "source": [
        "Über kaggle die eigene API herunterladen und uploaden (Hinweis: Account bei Kaggle und Google benötigt!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq_tJzDG28Z6"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files \n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "files.upload() #upload your kaggle.json file\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvGaUL3n5zYW"
      },
      "source": [
        "!mkdir ~/.kaggle #create a directory called .kaggle in the root folder\n",
        "!cp kaggle.json ~/.kaggle/ #copy kaggle.json to this folder\n",
        "!chmod 600 ~/.kaggle/kaggle.json #add full rights to this copied file\n",
        "!rm kaggle.json #remove the original one\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB8sjEjlJ2gx"
      },
      "source": [
        "\n",
        "**Datensatz über die Kaggle API downloaden**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rJd3yT8J-WN"
      },
      "source": [
        "!kaggle datasets download -d selfishgene/historical-hourly-weather-data #paste the kaggle API command\n",
        "!unzip -qq /content/historical-hourly-weather-data.zip #unzip the zip file\n",
        "!rm historical-hourly-weather-data.zip #remove the zip file\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQH6mfsjKo0C"
      },
      "source": [
        "**Check prerequisites**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R551dLMWKr1k"
      },
      "source": [
        "!pip install pandas #well known pandas library, used for data processing, wrangling .... by data scientists\n",
        "!pip install keras\n",
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrNInhqVK_VX"
      },
      "source": [
        "**2. Data processing** \n",
        "\n",
        "Identifizierung von Datenlücken"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6uf7LyRBaOt"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_temp = pd.read_csv('temperature.csv', usecols = ['datetime','Seattle'])\n",
        "df_temp['Seattle'] = df_temp['Seattle'].apply(lambda x: x-273.15) # umsrechnung von kelvin auf celcius\n",
        "df_humid = pd.read_csv('humidity.csv', usecols = ['datetime','Seattle'])\n",
        "df_pres = pd.read_csv('pressure.csv', usecols = ['datetime','Seattle'])\n",
        "df_desc = pd.read_csv('weather_description.csv', usecols = ['datetime','Seattle'])\n",
        "\n",
        "df_list = [df_temp, df_humid, df_pres, df_desc]\n",
        "\n",
        "for df in df_list:\n",
        "  df.drop(df.head(1).index, inplace=True)\n",
        "  df.reset_index(inplace=True, drop=True)\n",
        "  print(\"Count of nan values:\", df.isnull().values.sum())\n",
        "  print(\"Indexes of nan values:\", list(df.loc[pd.isna(df['Seattle']), :].index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWez1YNAB7Hh"
      },
      "source": [
        "Die Datenlücken linear befüllen (default von interpolate)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNTR-_ikBwFv"
      },
      "source": [
        "for df in df_list[:3]: \n",
        "  df.interpolate(inplace=True)\n",
        "  print(df)\n",
        "  print(\"Count of nan values:\", df.isnull().values.sum())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuyIJO-vFhpu"
      },
      "source": [
        "Die Klassen für die Klassifizierung definieren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y9h5l6dFfqz"
      },
      "source": [
        "weather_types = {'keinUnwetter':['few clouds', 'scattered clouds', 'broken clouds', 'overcast clouds',\n",
        "                                  'sky is clear','mist', 'haze', 'fog', 'smoke'],\n",
        "                 'regnerisch':['light rain', 'moderate rain','light intensity drizzle','drizzle',\n",
        "                               'heavy intensity drizzle', 'light intensity shower rain', 'shower rain',\n",
        "                               'light snow', 'snow','light shower snow'],\n",
        "                 'Unwetter':['heavy snow','heavy intensity rain', 'proximity thunderstorm',\n",
        "                                'thunderstorm with rain','thunderstorm with heavy rain', 'thunderstorm with light rain',\n",
        "                               'very heavy rain','heavy intensity shower rain', 'thunderstorm', 'squalls']\n",
        "                }\n",
        "\n",
        "def replace(content):\n",
        "  for weather_type in weather_types.keys():\n",
        "    for weather in weather_types[weather_type]:\n",
        "      if content == weather:      \n",
        "        return weather_type\n",
        "  print(content)\n",
        "\n",
        "df_desc['Seattle'] = df_desc['Seattle'].map(replace)  \n",
        "print(df_desc)\n",
        "\n",
        "labels_unique = df_desc.Seattle.unique()\n",
        "print(\"Wetter in Seattle:\", labels_unique)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fXpql0BSxwn"
      },
      "source": [
        "Die einzelnen Attributwerte zusammenbringen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr2K3nD9Sstc"
      },
      "source": [
        "#löscht die ersten 11 Zeilen und die letzte damit daten immer nur ein fenster von 24h haben\n",
        "df_temp = df_temp[11:-1].reset_index(drop=True)\n",
        "df_temp.rename(columns={'datetime':'timestamp','Seattle': 'temperatur'}, inplace=True)\n",
        "#print(df_temp)\n",
        "\n",
        "df_humid = df_humid[11:-1].reset_index(drop=True)\n",
        "df_humid.rename(columns={'datetime':'timestamp','Seattle': 'feuchtigkeit'}, inplace=True)\n",
        "#print(df_humid)\n",
        "\n",
        "df_pres = df_pres[11:-1].reset_index(drop=True)\n",
        "df_pres.rename(columns={'datetime':'timestamp','Seattle': 'druck'}, inplace=True)\n",
        "#print(df_pres)\n",
        "\n",
        "df_desc = df_desc[11:-1].reset_index(drop=True)\n",
        "df_desc.rename(columns={'datetime':'timestamp','Seattle': 'wetter'}, inplace=True)\n",
        "#print(df_desc)\n",
        "\n",
        "#left join\n",
        "df = df_temp.merge(df_humid, how='left', on=['timestamp']).merge(df_pres, how='left', on=['timestamp']).merge(df_desc,\n",
        "                                                                                                            how='left',\n",
        "                                                                                                            on=['timestamp'])\n",
        "\n",
        "print(\"First 50 rows: \\n\", df[0:50])\n",
        "print(\"Last 50 rows: \\n\", df[-50:])\n",
        "\n",
        "#final check for null values\n",
        "print(df['temperatur'].isnull().sum())\n",
        "print(df['feuchtigkeit'].isnull().sum())\n",
        "print(df['druck'].isnull().sum())\n",
        "print(df['wetter'].isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyyj6I2V0Csn"
      },
      "source": [
        "Statistik der Trainingsdaten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIVQcwRVkCiS"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "top_types = df['wetter'].value_counts()\n",
        "print(top_types ,\"\\n\", df.value_counts)\n",
        "df['wetter'].value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"yellow\", \"green\", \"red\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrab47Yh8tcF"
      },
      "source": [
        "**3. Unser Modell trainieren**\n",
        "\n",
        "Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMmBhvrWCwo-"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "df_model = df[['temperatur', 'feuchtigkeit', 'druck', 'wetter']]\n",
        "df_model['wetter'] = df_model['wetter'].replace(['regnerisch', 'Unwetter', 'keinUnwetter'],[0,1,2])\n",
        "df_model['wetter'] = df_model['wetter'].astype(int)\n",
        "\n",
        "labels = to_categorical(df_model.pop('wetter')) #Create classes from the labels\n",
        "features = np.array(df_model) #convert our dataframe into ndarray, only array type that neural network takes as input\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.15,shuffle=True)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7-Fmxl887Z3"
      },
      "source": [
        "Das neurale Netzwerk definieren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xQzV7sx867v"
      },
      "source": [
        "#Parameters:\n",
        "NB_classes = 3 #number of outputs\n",
        "NB_neurones = 30 #main number of neurones\n",
        "NB_features = 3 #number of inputs\n",
        "activation_func = tf.keras.activations.relu #activation function used\n",
        "\n",
        "#Densly connected neural network\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func,input_shape=(NB_features,)),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dense(NB_neurones,activation=activation_func),\n",
        "                             tf.keras.layers.Dropout(0.4), #avoid overfiting\n",
        "                             #softmax will output an array containing probabilities of each classes\n",
        "                             #the highest one is the predicted class\n",
        "                             tf.keras.layers.Dense(NB_classes,activation=tf.keras.activations.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\",loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy']) #compile the model\n",
        "\n",
        "model.summary() #to see the paramter of our model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCQKGPM39j-3"
      },
      "source": [
        "Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgTN2TCV9mUz"
      },
      "source": [
        "model.fit(x=train_features,\n",
        "          y=train_labels,\n",
        "          epochs=20,\n",
        "          validation_data=(test_features,test_labels),\n",
        "          verbose=1,\n",
        "          shuffle=True) #Train our model\n",
        "\n",
        "\n",
        "performance=model.evaluate(test_features,test_labels, batch_size=32, verbose=1, steps=None, )[1] * 100\n",
        "print('Final accuracy : ', round(performance), '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ-3wmUn9328"
      },
      "source": [
        "**4. Konvertierung des Modells für das deployment auf den Arduino**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_r8JfQF93DR"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model) #create a converter\n",
        "tflite_model = converter.convert() #convert the model without quantization \n",
        "\n",
        "\n",
        "open(\"/content/tflite_model.tflite\",\"wb\").write(tflite_model) #Create a file containing our tflite model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZKMz-40biqH"
      },
      "source": [
        "!apt-get install -qq xxd #installing the tool\n",
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat /content/tflite_model.tflite | xxd -i >> /content/model.h #create an hexadecimal array containing all our parameters\n",
        "!echo \"};\" >> /content/model.h\n",
        "\n",
        "files.download(\"/content/model.h\") #automaticly download your file"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}